{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi1KA6tLfa76"
      },
      "source": [
        "# LIBRARIES AND REQUIREMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQapSIwd3slX",
        "outputId": "65352670-2965-42d8-e34b-b95c71a9fe59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if os.path.exists('Detic'):\n",
        "  print(\"Drive Mounted\")\n",
        "else:\n",
        "  drive.mount('/content/drive/', force_remount=True)\n",
        "  !unzip -qq /content/drive/MyDrive/INTERPRETABILITY/Zip\\ Files/Detic.zip -d /\n",
        "  !unzip -qq /content/drive/MyDrive/INTERPRETABILITY/Zip\\ Files/detectron2.zip -d /\n",
        "  !unzip -qq /content/drive/MyDrive/INTERPRETABILITY/Zip\\ Files/Interpretability.zip -d /content/\n",
        "  !unzip -qq /content/drive/MyDrive/INTERPRETABILITY/Zip\\ Files/in_painting.zip -d /content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9d4aq0eR7YOy"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/Interpretability\")\n",
        "outer_path = \"/content/Interpretability/\"\n",
        "\n",
        "# if os.path.exists('/content/classes'):\n",
        "#   print(\"Unzipped dataset\")\n",
        "# else:\n",
        "#   !unzip -qq /content/Interpretability/datasets/LigiLog-100_coco_classes.zip -d /content/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7MpTaZ0YNTb",
        "outputId": "82dcd533-5d82-4bbf-c5a4-4e4ba86c0b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 151 kB 72.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 64.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 509 kB 73.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 248 kB 76.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 96 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 843 kB 60.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 117 kB 71.7 MB/s \n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.7 MB/s \n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install detectron2\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "# print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "!pip install click==7.1.2\n",
        "\n",
        "!pip install -q  -e detectron2\n",
        "\n",
        "# clone and install Detic\n",
        "!pip install -q  -r Detic/requirements.txt\n",
        "!pip install -q -r Detic/third_party/CenterNet2/requirements.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LER_mry-ZFYC",
        "outputId": "6aa2a438-53d7-4791-8d85-bf7e355f94a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.6.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchsort\n",
            "  Downloading torchsort-0.1.9.tar.gz (12 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsort) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsort) (4.1.1)\n",
            "Building wheels for collected packages: torchsort\n",
            "  Building wheel for torchsort (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchsort: filename=torchsort-0.1.9-cp37-cp37m-linux_x86_64.whl size=2768795 sha256=22b8f42f9b3b59b83f49454080096f910f063aa850a837b6875da89fdbeb3727\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/20/69/cc894d3d65cb58f99ada9462cd8fb76e3beaac185efa06df3e\n",
            "Successfully built torchsort\n",
            "Installing collected packages: torchsort\n",
            "Successfully installed torchsort-0.1.9\n"
          ]
        }
      ],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import sys\n",
        "outer_path = \"/content/detectron2/\"\n",
        "sys.path.append(outer_path)\n",
        "from PIL import Image\n",
        "import detectron2 \n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "from google.colab.patches import cv2_imshow\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "# import some common libraries\n",
        "import sys\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import importlib\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "# Detic libraries\n",
        "sys.path.insert(0, '/content/Detic/third_party/CenterNet2')\n",
        "from centernet.config import add_centernet_config\n",
        "from Detic.detic.config import add_detic_config\n",
        "from Detic.detic.modeling.utils import reset_cls_test\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import os\n",
        "import cv2\n",
        "\n",
        "try:\n",
        "  import models as md\n",
        "except:\n",
        "  !pip install timm\n",
        "  !pip install torchsort\n",
        "  import models as md\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import torch as tr\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torch.nn.functional import interpolate\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from scipy.stats import percentileofscore\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "device = tr.device(\"cuda:0\" if tr.cuda.is_available() else \"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fvv6dtFfBMU",
        "outputId": "04c6b882-b7ed-425b-d87f-b196d54e5598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-2s9498ad\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-2s9498ad\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "import clip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWn8Z-9ZfeV5"
      },
      "source": [
        "# OBJECT SEGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCGX5qP5fhya",
        "outputId": "3d68b849-7bfa-40c0-9a39-97d60d371443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth: 702MB [00:35, 19.6MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[09/13 14:36:18 d2.checkpoint.c2_model_loading]: \u001b[0mFollowing weights matched with model:\n",
            "| Names in Model                                          | Names in Checkpoint                                                                                                                         | Shapes                                                   |\n",
            "|:--------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------|\n",
            "| backbone.bottom_up.layers.0.blocks.0.attn.*             | backbone.bottom_up.layers.0.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (128,) (128,128) (384,) (384,128) (169,4) (49,49)        |\n",
            "| backbone.bottom_up.layers.0.blocks.0.mlp.fc1.*          | backbone.bottom_up.layers.0.blocks.0.mlp.fc1.{bias,weight}                                                                                  | (512,) (512,128)                                         |\n",
            "| backbone.bottom_up.layers.0.blocks.0.mlp.fc2.*          | backbone.bottom_up.layers.0.blocks.0.mlp.fc2.{bias,weight}                                                                                  | (128,) (128,512)                                         |\n",
            "| backbone.bottom_up.layers.0.blocks.0.norm1.*            | backbone.bottom_up.layers.0.blocks.0.norm1.{bias,weight}                                                                                    | (128,) (128,)                                            |\n",
            "| backbone.bottom_up.layers.0.blocks.0.norm2.*            | backbone.bottom_up.layers.0.blocks.0.norm2.{bias,weight}                                                                                    | (128,) (128,)                                            |\n",
            "| backbone.bottom_up.layers.0.blocks.1.attn.*             | backbone.bottom_up.layers.0.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (128,) (128,128) (384,) (384,128) (169,4) (49,49)        |\n",
            "| backbone.bottom_up.layers.0.blocks.1.mlp.fc1.*          | backbone.bottom_up.layers.0.blocks.1.mlp.fc1.{bias,weight}                                                                                  | (512,) (512,128)                                         |\n",
            "| backbone.bottom_up.layers.0.blocks.1.mlp.fc2.*          | backbone.bottom_up.layers.0.blocks.1.mlp.fc2.{bias,weight}                                                                                  | (128,) (128,512)                                         |\n",
            "| backbone.bottom_up.layers.0.blocks.1.norm1.*            | backbone.bottom_up.layers.0.blocks.1.norm1.{bias,weight}                                                                                    | (128,) (128,)                                            |\n",
            "| backbone.bottom_up.layers.0.blocks.1.norm2.*            | backbone.bottom_up.layers.0.blocks.1.norm2.{bias,weight}                                                                                    | (128,) (128,)                                            |\n",
            "| backbone.bottom_up.layers.0.downsample.norm.*           | backbone.bottom_up.layers.0.downsample.norm.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.0.downsample.reduction.weight | backbone.bottom_up.layers.0.downsample.reduction.weight                                                                                     | (256, 512)                                               |\n",
            "| backbone.bottom_up.layers.1.blocks.0.attn.*             | backbone.bottom_up.layers.1.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (256,) (256,256) (768,) (768,256) (169,8) (49,49)        |\n",
            "| backbone.bottom_up.layers.1.blocks.0.mlp.fc1.*          | backbone.bottom_up.layers.1.blocks.0.mlp.fc1.{bias,weight}                                                                                  | (1024,) (1024,256)                                       |\n",
            "| backbone.bottom_up.layers.1.blocks.0.mlp.fc2.*          | backbone.bottom_up.layers.1.blocks.0.mlp.fc2.{bias,weight}                                                                                  | (256,) (256,1024)                                        |\n",
            "| backbone.bottom_up.layers.1.blocks.0.norm1.*            | backbone.bottom_up.layers.1.blocks.0.norm1.{bias,weight}                                                                                    | (256,) (256,)                                            |\n",
            "| backbone.bottom_up.layers.1.blocks.0.norm2.*            | backbone.bottom_up.layers.1.blocks.0.norm2.{bias,weight}                                                                                    | (256,) (256,)                                            |\n",
            "| backbone.bottom_up.layers.1.blocks.1.attn.*             | backbone.bottom_up.layers.1.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (256,) (256,256) (768,) (768,256) (169,8) (49,49)        |\n",
            "| backbone.bottom_up.layers.1.blocks.1.mlp.fc1.*          | backbone.bottom_up.layers.1.blocks.1.mlp.fc1.{bias,weight}                                                                                  | (1024,) (1024,256)                                       |\n",
            "| backbone.bottom_up.layers.1.blocks.1.mlp.fc2.*          | backbone.bottom_up.layers.1.blocks.1.mlp.fc2.{bias,weight}                                                                                  | (256,) (256,1024)                                        |\n",
            "| backbone.bottom_up.layers.1.blocks.1.norm1.*            | backbone.bottom_up.layers.1.blocks.1.norm1.{bias,weight}                                                                                    | (256,) (256,)                                            |\n",
            "| backbone.bottom_up.layers.1.blocks.1.norm2.*            | backbone.bottom_up.layers.1.blocks.1.norm2.{bias,weight}                                                                                    | (256,) (256,)                                            |\n",
            "| backbone.bottom_up.layers.1.downsample.norm.*           | backbone.bottom_up.layers.1.downsample.norm.{bias,weight}                                                                                   | (1024,) (1024,)                                          |\n",
            "| backbone.bottom_up.layers.1.downsample.reduction.weight | backbone.bottom_up.layers.1.downsample.reduction.weight                                                                                     | (512, 1024)                                              |\n",
            "| backbone.bottom_up.layers.2.blocks.0.attn.*             | backbone.bottom_up.layers.2.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.0.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.0.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.0.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.0.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.0.norm1.*            | backbone.bottom_up.layers.2.blocks.0.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.0.norm2.*            | backbone.bottom_up.layers.2.blocks.0.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.1.attn.*             | backbone.bottom_up.layers.2.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.1.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.1.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.1.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.1.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.1.norm1.*            | backbone.bottom_up.layers.2.blocks.1.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.1.norm2.*            | backbone.bottom_up.layers.2.blocks.1.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.10.attn.*            | backbone.bottom_up.layers.2.blocks.10.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.10.mlp.fc1.*         | backbone.bottom_up.layers.2.blocks.10.mlp.fc1.{bias,weight}                                                                                 | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.10.mlp.fc2.*         | backbone.bottom_up.layers.2.blocks.10.mlp.fc2.{bias,weight}                                                                                 | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.10.norm1.*           | backbone.bottom_up.layers.2.blocks.10.norm1.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.10.norm2.*           | backbone.bottom_up.layers.2.blocks.10.norm2.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.11.attn.*            | backbone.bottom_up.layers.2.blocks.11.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.11.mlp.fc1.*         | backbone.bottom_up.layers.2.blocks.11.mlp.fc1.{bias,weight}                                                                                 | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.11.mlp.fc2.*         | backbone.bottom_up.layers.2.blocks.11.mlp.fc2.{bias,weight}                                                                                 | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.11.norm1.*           | backbone.bottom_up.layers.2.blocks.11.norm1.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.11.norm2.*           | backbone.bottom_up.layers.2.blocks.11.norm2.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.12.attn.*            | backbone.bottom_up.layers.2.blocks.12.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.12.mlp.fc1.*         | backbone.bottom_up.layers.2.blocks.12.mlp.fc1.{bias,weight}                                                                                 | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.12.mlp.fc2.*         | backbone.bottom_up.layers.2.blocks.12.mlp.fc2.{bias,weight}                                                                                 | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.12.norm1.*           | backbone.bottom_up.layers.2.blocks.12.norm1.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.12.norm2.*           | backbone.bottom_up.layers.2.blocks.12.norm2.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.13.attn.*            | backbone.bottom_up.layers.2.blocks.13.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.13.mlp.fc1.*         | backbone.bottom_up.layers.2.blocks.13.mlp.fc1.{bias,weight}                                                                                 | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.13.mlp.fc2.*         | backbone.bottom_up.layers.2.blocks.13.mlp.fc2.{bias,weight}                                                                                 | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.13.norm1.*           | backbone.bottom_up.layers.2.blocks.13.norm1.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.13.norm2.*           | backbone.bottom_up.layers.2.blocks.13.norm2.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.14.attn.*            | backbone.bottom_up.layers.2.blocks.14.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.14.mlp.fc1.*         | backbone.bottom_up.layers.2.blocks.14.mlp.fc1.{bias,weight}                                                                                 | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.14.mlp.fc2.*         | backbone.bottom_up.layers.2.blocks.14.mlp.fc2.{bias,weight}                                                                                 | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.14.norm1.*           | backbone.bottom_up.layers.2.blocks.14.norm1.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.14.norm2.*           | backbone.bottom_up.layers.2.blocks.14.norm2.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.15.attn.*            | backbone.bottom_up.layers.2.blocks.15.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.15.mlp.fc1.*         | backbone.bottom_up.layers.2.blocks.15.mlp.fc1.{bias,weight}                                                                                 | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.15.mlp.fc2.*         | backbone.bottom_up.layers.2.blocks.15.mlp.fc2.{bias,weight}                                                                                 | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.15.norm1.*           | backbone.bottom_up.layers.2.blocks.15.norm1.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.15.norm2.*           | backbone.bottom_up.layers.2.blocks.15.norm2.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.16.attn.*            | backbone.bottom_up.layers.2.blocks.16.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.16.mlp.fc1.*         | backbone.bottom_up.layers.2.blocks.16.mlp.fc1.{bias,weight}                                                                                 | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.16.mlp.fc2.*         | backbone.bottom_up.layers.2.blocks.16.mlp.fc2.{bias,weight}                                                                                 | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.16.norm1.*           | backbone.bottom_up.layers.2.blocks.16.norm1.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.16.norm2.*           | backbone.bottom_up.layers.2.blocks.16.norm2.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.17.attn.*            | backbone.bottom_up.layers.2.blocks.17.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.17.mlp.fc1.*         | backbone.bottom_up.layers.2.blocks.17.mlp.fc1.{bias,weight}                                                                                 | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.17.mlp.fc2.*         | backbone.bottom_up.layers.2.blocks.17.mlp.fc2.{bias,weight}                                                                                 | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.17.norm1.*           | backbone.bottom_up.layers.2.blocks.17.norm1.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.17.norm2.*           | backbone.bottom_up.layers.2.blocks.17.norm2.{bias,weight}                                                                                   | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.2.attn.*             | backbone.bottom_up.layers.2.blocks.2.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.2.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.2.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.2.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.2.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.2.norm1.*            | backbone.bottom_up.layers.2.blocks.2.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.2.norm2.*            | backbone.bottom_up.layers.2.blocks.2.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.3.attn.*             | backbone.bottom_up.layers.2.blocks.3.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.3.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.3.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.3.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.3.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.3.norm1.*            | backbone.bottom_up.layers.2.blocks.3.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.3.norm2.*            | backbone.bottom_up.layers.2.blocks.3.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.4.attn.*             | backbone.bottom_up.layers.2.blocks.4.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.4.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.4.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.4.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.4.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.4.norm1.*            | backbone.bottom_up.layers.2.blocks.4.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.4.norm2.*            | backbone.bottom_up.layers.2.blocks.4.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.5.attn.*             | backbone.bottom_up.layers.2.blocks.5.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.5.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.5.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.5.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.5.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.5.norm1.*            | backbone.bottom_up.layers.2.blocks.5.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.5.norm2.*            | backbone.bottom_up.layers.2.blocks.5.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.6.attn.*             | backbone.bottom_up.layers.2.blocks.6.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.6.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.6.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.6.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.6.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.6.norm1.*            | backbone.bottom_up.layers.2.blocks.6.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.6.norm2.*            | backbone.bottom_up.layers.2.blocks.6.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.7.attn.*             | backbone.bottom_up.layers.2.blocks.7.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.7.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.7.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.7.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.7.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.7.norm1.*            | backbone.bottom_up.layers.2.blocks.7.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.7.norm2.*            | backbone.bottom_up.layers.2.blocks.7.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.8.attn.*             | backbone.bottom_up.layers.2.blocks.8.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.8.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.8.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.8.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.8.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.8.norm1.*            | backbone.bottom_up.layers.2.blocks.8.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.8.norm2.*            | backbone.bottom_up.layers.2.blocks.8.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.9.attn.*             | backbone.bottom_up.layers.2.blocks.9.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (512,) (512,512) (1536,) (1536,512) (169,16) (49,49)     |\n",
            "| backbone.bottom_up.layers.2.blocks.9.mlp.fc1.*          | backbone.bottom_up.layers.2.blocks.9.mlp.fc1.{bias,weight}                                                                                  | (2048,) (2048,512)                                       |\n",
            "| backbone.bottom_up.layers.2.blocks.9.mlp.fc2.*          | backbone.bottom_up.layers.2.blocks.9.mlp.fc2.{bias,weight}                                                                                  | (512,) (512,2048)                                        |\n",
            "| backbone.bottom_up.layers.2.blocks.9.norm1.*            | backbone.bottom_up.layers.2.blocks.9.norm1.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.blocks.9.norm2.*            | backbone.bottom_up.layers.2.blocks.9.norm2.{bias,weight}                                                                                    | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.layers.2.downsample.norm.*           | backbone.bottom_up.layers.2.downsample.norm.{bias,weight}                                                                                   | (2048,) (2048,)                                          |\n",
            "| backbone.bottom_up.layers.2.downsample.reduction.weight | backbone.bottom_up.layers.2.downsample.reduction.weight                                                                                     | (1024, 2048)                                             |\n",
            "| backbone.bottom_up.layers.3.blocks.0.attn.*             | backbone.bottom_up.layers.3.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (1024,) (1024,1024) (3072,) (3072,1024) (169,32) (49,49) |\n",
            "| backbone.bottom_up.layers.3.blocks.0.mlp.fc1.*          | backbone.bottom_up.layers.3.blocks.0.mlp.fc1.{bias,weight}                                                                                  | (4096,) (4096,1024)                                      |\n",
            "| backbone.bottom_up.layers.3.blocks.0.mlp.fc2.*          | backbone.bottom_up.layers.3.blocks.0.mlp.fc2.{bias,weight}                                                                                  | (1024,) (1024,4096)                                      |\n",
            "| backbone.bottom_up.layers.3.blocks.0.norm1.*            | backbone.bottom_up.layers.3.blocks.0.norm1.{bias,weight}                                                                                    | (1024,) (1024,)                                          |\n",
            "| backbone.bottom_up.layers.3.blocks.0.norm2.*            | backbone.bottom_up.layers.3.blocks.0.norm2.{bias,weight}                                                                                    | (1024,) (1024,)                                          |\n",
            "| backbone.bottom_up.layers.3.blocks.1.attn.*             | backbone.bottom_up.layers.3.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (1024,) (1024,1024) (3072,) (3072,1024) (169,32) (49,49) |\n",
            "| backbone.bottom_up.layers.3.blocks.1.mlp.fc1.*          | backbone.bottom_up.layers.3.blocks.1.mlp.fc1.{bias,weight}                                                                                  | (4096,) (4096,1024)                                      |\n",
            "| backbone.bottom_up.layers.3.blocks.1.mlp.fc2.*          | backbone.bottom_up.layers.3.blocks.1.mlp.fc2.{bias,weight}                                                                                  | (1024,) (1024,4096)                                      |\n",
            "| backbone.bottom_up.layers.3.blocks.1.norm1.*            | backbone.bottom_up.layers.3.blocks.1.norm1.{bias,weight}                                                                                    | (1024,) (1024,)                                          |\n",
            "| backbone.bottom_up.layers.3.blocks.1.norm2.*            | backbone.bottom_up.layers.3.blocks.1.norm2.{bias,weight}                                                                                    | (1024,) (1024,)                                          |\n",
            "| backbone.bottom_up.norm1.*                              | backbone.bottom_up.norm1.{bias,weight}                                                                                                      | (256,) (256,)                                            |\n",
            "| backbone.bottom_up.norm2.*                              | backbone.bottom_up.norm2.{bias,weight}                                                                                                      | (512,) (512,)                                            |\n",
            "| backbone.bottom_up.norm3.*                              | backbone.bottom_up.norm3.{bias,weight}                                                                                                      | (1024,) (1024,)                                          |\n",
            "| backbone.bottom_up.patch_embed.norm.*                   | backbone.bottom_up.patch_embed.norm.{bias,weight}                                                                                           | (128,) (128,)                                            |\n",
            "| backbone.bottom_up.patch_embed.proj.*                   | backbone.bottom_up.patch_embed.proj.{bias,weight}                                                                                           | (128,) (128,3,4,4)                                       |\n",
            "| backbone.fpn_lateral3.*                                 | backbone.fpn_lateral3.{bias,weight}                                                                                                         | (256,) (256,256,1,1)                                     |\n",
            "| backbone.fpn_lateral4.*                                 | backbone.fpn_lateral4.{bias,weight}                                                                                                         | (256,) (256,512,1,1)                                     |\n",
            "| backbone.fpn_lateral5.*                                 | backbone.fpn_lateral5.{bias,weight}                                                                                                         | (256,) (256,1024,1,1)                                    |\n",
            "| backbone.fpn_output3.*                                  | backbone.fpn_output3.{bias,weight}                                                                                                          | (256,) (256,256,3,3)                                     |\n",
            "| backbone.fpn_output4.*                                  | backbone.fpn_output4.{bias,weight}                                                                                                          | (256,) (256,256,3,3)                                     |\n",
            "| backbone.fpn_output5.*                                  | backbone.fpn_output5.{bias,weight}                                                                                                          | (256,) (256,256,3,3)                                     |\n",
            "| backbone.top_block.p6.*                                 | backbone.top_block.p6.{bias,weight}                                                                                                         | (256,) (256,256,3,3)                                     |\n",
            "| backbone.top_block.p7.*                                 | backbone.top_block.p7.{bias,weight}                                                                                                         | (256,) (256,256,3,3)                                     |\n",
            "| proposal_generator.centernet_head.agn_hm.*              | proposal_generator.centernet_head.agn_hm.{bias,weight}                                                                                      | (1,) (1,256,3,3)                                         |\n",
            "| proposal_generator.centernet_head.bbox_pred.*           | proposal_generator.centernet_head.bbox_pred.{bias,weight}                                                                                   | (4,) (4,256,3,3)                                         |\n",
            "| proposal_generator.centernet_head.bbox_tower.0.*        | proposal_generator.centernet_head.bbox_tower.0.{bias,weight}                                                                                | (256,) (256,256,3,3)                                     |\n",
            "| proposal_generator.centernet_head.bbox_tower.1.*        | proposal_generator.centernet_head.bbox_tower.1.{bias,weight}                                                                                | (256,) (256,)                                            |\n",
            "| proposal_generator.centernet_head.bbox_tower.10.*       | proposal_generator.centernet_head.bbox_tower.10.{bias,weight}                                                                               | (256,) (256,)                                            |\n",
            "| proposal_generator.centernet_head.bbox_tower.3.*        | proposal_generator.centernet_head.bbox_tower.3.{bias,weight}                                                                                | (256,) (256,256,3,3)                                     |\n",
            "| proposal_generator.centernet_head.bbox_tower.4.*        | proposal_generator.centernet_head.bbox_tower.4.{bias,weight}                                                                                | (256,) (256,)                                            |\n",
            "| proposal_generator.centernet_head.bbox_tower.6.*        | proposal_generator.centernet_head.bbox_tower.6.{bias,weight}                                                                                | (256,) (256,256,3,3)                                     |\n",
            "| proposal_generator.centernet_head.bbox_tower.7.*        | proposal_generator.centernet_head.bbox_tower.7.{bias,weight}                                                                                | (256,) (256,)                                            |\n",
            "| proposal_generator.centernet_head.bbox_tower.9.*        | proposal_generator.centernet_head.bbox_tower.9.{bias,weight}                                                                                | (256,) (256,256,3,3)                                     |\n",
            "| proposal_generator.centernet_head.scales.0.scale        | proposal_generator.centernet_head.scales.0.scale                                                                                            | (1,)                                                     |\n",
            "| proposal_generator.centernet_head.scales.1.scale        | proposal_generator.centernet_head.scales.1.scale                                                                                            | (1,)                                                     |\n",
            "| proposal_generator.centernet_head.scales.2.scale        | proposal_generator.centernet_head.scales.2.scale                                                                                            | (1,)                                                     |\n",
            "| proposal_generator.centernet_head.scales.3.scale        | proposal_generator.centernet_head.scales.3.scale                                                                                            | (1,)                                                     |\n",
            "| proposal_generator.centernet_head.scales.4.scale        | proposal_generator.centernet_head.scales.4.scale                                                                                            | (1,)                                                     |\n",
            "| roi_heads.box_head.0.fc1.*                              | roi_heads.box_head.0.fc1.{bias,weight}                                                                                                      | (1024,) (1024,12544)                                     |\n",
            "| roi_heads.box_head.0.fc2.*                              | roi_heads.box_head.0.fc2.{bias,weight}                                                                                                      | (1024,) (1024,1024)                                      |\n",
            "| roi_heads.box_head.1.fc1.*                              | roi_heads.box_head.1.fc1.{bias,weight}                                                                                                      | (1024,) (1024,12544)                                     |\n",
            "| roi_heads.box_head.1.fc2.*                              | roi_heads.box_head.1.fc2.{bias,weight}                                                                                                      | (1024,) (1024,1024)                                      |\n",
            "| roi_heads.box_head.2.fc1.*                              | roi_heads.box_head.2.fc1.{bias,weight}                                                                                                      | (1024,) (1024,12544)                                     |\n",
            "| roi_heads.box_head.2.fc2.*                              | roi_heads.box_head.2.fc2.{bias,weight}                                                                                                      | (1024,) (1024,1024)                                      |\n",
            "| roi_heads.box_predictor.0.bbox_pred.0.*                 | roi_heads.box_predictor.0.bbox_pred.0.{bias,weight}                                                                                         | (1024,) (1024,1024)                                      |\n",
            "| roi_heads.box_predictor.0.bbox_pred.2.*                 | roi_heads.box_predictor.0.bbox_pred.2.{bias,weight}                                                                                         | (4,) (4,1024)                                            |\n",
            "| roi_heads.box_predictor.0.cls_score.*                   | roi_heads.box_predictor.0.cls_score.{linear.bias,linear.weight,zs_weight}                                                                   | (512,) (512,1024) (512,22048)                            |\n",
            "| roi_heads.box_predictor.1.bbox_pred.0.*                 | roi_heads.box_predictor.1.bbox_pred.0.{bias,weight}                                                                                         | (1024,) (1024,1024)                                      |\n",
            "| roi_heads.box_predictor.1.bbox_pred.2.*                 | roi_heads.box_predictor.1.bbox_pred.2.{bias,weight}                                                                                         | (4,) (4,1024)                                            |\n",
            "| roi_heads.box_predictor.1.cls_score.*                   | roi_heads.box_predictor.1.cls_score.{linear.bias,linear.weight,zs_weight}                                                                   | (512,) (512,1024) (512,22048)                            |\n",
            "| roi_heads.box_predictor.2.bbox_pred.0.*                 | roi_heads.box_predictor.2.bbox_pred.0.{bias,weight}                                                                                         | (1024,) (1024,1024)                                      |\n",
            "| roi_heads.box_predictor.2.bbox_pred.2.*                 | roi_heads.box_predictor.2.bbox_pred.2.{bias,weight}                                                                                         | (4,) (4,1024)                                            |\n",
            "| roi_heads.box_predictor.2.cls_score.*                   | roi_heads.box_predictor.2.cls_score.{linear.bias,linear.weight,zs_weight}                                                                   | (512,) (512,1024) (512,22048)                            |\n",
            "| roi_heads.mask_head.deconv.*                            | roi_heads.mask_head.deconv.{bias,weight}                                                                                                    | (256,) (256,256,2,2)                                     |\n",
            "| roi_heads.mask_head.mask_fcn1.*                         | roi_heads.mask_head.mask_fcn1.{bias,weight}                                                                                                 | (256,) (256,256,3,3)                                     |\n",
            "| roi_heads.mask_head.mask_fcn2.*                         | roi_heads.mask_head.mask_fcn2.{bias,weight}                                                                                                 | (256,) (256,256,3,3)                                     |\n",
            "| roi_heads.mask_head.mask_fcn3.*                         | roi_heads.mask_head.mask_fcn3.{bias,weight}                                                                                                 | (256,) (256,256,3,3)                                     |\n",
            "| roi_heads.mask_head.mask_fcn4.*                         | roi_heads.mask_head.mask_fcn4.{bias,weight}                                                                                                 | (256,) (256,256,3,3)                                     |\n",
            "| roi_heads.mask_head.predictor.*                         | roi_heads.mask_head.predictor.{bias,weight}                                                                                                 | (1,) (1,256,1,1)                                         |\n",
            "Resetting zs_weight Detic/datasets/metadata/lvis_v1_clip_a+cname.npy\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_predictor():\n",
        "\t# Build the detector and download our pretrained weights\n",
        "\tcfg = get_cfg()\n",
        "\tadd_centernet_config(cfg)\n",
        "\tadd_detic_config(cfg)\n",
        "\tcfg.merge_from_file(\"/content/Detic/configs/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.yaml\")\n",
        "\tcfg.MODEL.WEIGHTS = 'https://dl.fbaipublicfiles.com/detic/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth'\n",
        "\tcfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "\tcfg.MODEL.ROI_BOX_HEAD.ZEROSHOT_WEIGHT_PATH = 'rand'\n",
        "\tcfg.MODEL.ROI_HEADS.ONE_CLASS_PER_PROPOSAL = True # For better visualization purpose. Set to False for all classes.\n",
        "\t# cfg.MODEL.DEVICE='cpu' # uncomment this to use cpu-only mode.\n",
        "\tpredictor = DefaultPredictor(cfg)\n",
        "\n",
        "\tBUILDIN_CLASSIFIER = {\n",
        "\t    'lvis': 'Detic/datasets/metadata/lvis_v1_clip_a+cname.npy',\n",
        "\t    'objects365': 'Detic/datasets/metadata/o365_clip_a+cnamefix.npy',\n",
        "\t    'openimages': 'Detic/datasets/metadata/oid_clip_a+cname.npy',\n",
        "\t    'coco': 'Detic/datasets/metadata/coco_clip_a+cname.npy',\n",
        "\t}\n",
        "\n",
        "\tBUILDIN_METADATA_PATH = {\n",
        "\t    'lvis': 'lvis_v1_val',\n",
        "\t    'objects365': 'objects365_v2_val',\n",
        "\t    'openimages': 'oid_val_expanded',\n",
        "\t    'coco': 'coco_2017_val',\n",
        "\t}\n",
        "\n",
        "\tvocabulary = 'lvis' # change to 'lvis', 'objects365', 'openimages', or 'coco'\n",
        "\tmetadata = MetadataCatalog.get(BUILDIN_METADATA_PATH[vocabulary])\n",
        "\tclassifier = BUILDIN_CLASSIFIER[vocabulary]\n",
        "\tnum_classes = len(metadata.thing_classes)\n",
        "\treset_cls_test(predictor.model, classifier, num_classes)\n",
        "\treturn predictor, metadata\n",
        "\n",
        "\n",
        "predictor, metadata = get_predictor()\n",
        "\n",
        "\n",
        "def segmentation(im_name, input_type=\"img\"):\n",
        "\t\n",
        "\tif input_type!=\"img\":\n",
        "\t\tim = cv2.imread(im_name)\n",
        "\t\tim = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "\t \n",
        "\toutputs = predictor(im)\n",
        "\tclasses = outputs[\"instances\"].to(\"cpu\").pred_classes.tolist()\n",
        "\tclass_names = [metadata.thing_classes[cl] for cl in classes]\n",
        "\treturn outputs, class_names\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxBOq5AlfkUt"
      },
      "source": [
        "# MEMORABILITY MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPt9gxCBfmD3",
        "outputId": "3caa1c41-8064-404d-f31b-529e651882ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ns-6f26d0cf.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b5_ns-6f26d0cf.pth\n"
          ]
        }
      ],
      "source": [
        "model = md.EfficientNet()\n",
        "\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "name_of_model = \"baseline_2021-12-20\"\n",
        "if os.path.isfile(outer_path+\"parameters/\"+name_of_model+\".pmt\"):\n",
        "    # print(\"Loading parameters from file\")\n",
        "    model.load_state_dict(tr.load(outer_path+\"parameters/\"+name_of_model+\".pmt\"))\n",
        "\n",
        "model.to(device);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-e5t3yGfqf_"
      },
      "source": [
        "# IMAGE TRANSFORMATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4IkSyB_hfp-U"
      },
      "outputs": [],
      "source": [
        "def ImageTransform(path, ip_type=\"filename\"):\n",
        "  if ip_type==\"filename\":\n",
        "    file_name = path\n",
        "    img = Image.open(file_name)\n",
        "    img = img.convert(\"RGB\")\n",
        "\n",
        "  elif ip_type==\"otr\":\n",
        "    img = Image.fromarray(path)\n",
        "    \n",
        "  else:\n",
        "    img = path\n",
        "\n",
        "  transform1 = T.Compose([T.ToTensor()])\n",
        "\n",
        "  x = transform1(img)*255\n",
        "\n",
        "  img_mean = tr.Tensor([123.675, 116.28, 103.53]).view(3, 1, 1)\n",
        "  img_std = tr.Tensor([58.395, 57.12, 57.375]).view(3, 1, 1)\n",
        "\n",
        "  x = x.to(dtype=tr.float)\n",
        "\n",
        "  x = interpolate(\n",
        "      x.unsqueeze(0),\n",
        "      size=(456,456),#(model.img_size, model.img_size),\n",
        "      mode='bilinear',\n",
        "      align_corners=False,\n",
        "      recompute_scale_factor=False\n",
        "  ).squeeze(0)\n",
        "\n",
        "  x = ((x - img_mean) / img_std)#.unsqueeze(0)\n",
        "  return np.array(img), x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2ctnZ4nAfvEk"
      },
      "outputs": [],
      "source": [
        "def alter_mask(mask, val = 50):\n",
        "  # mask = Image.fromarray(mask)\n",
        "  # mask = mask[:, :, ::-1].copy() \n",
        "  plt.imsave(\"temp_mask.png\", mask)\n",
        "  mask = cv2.imread(\"temp_mask.png\", cv2.THRESH_BINARY)\n",
        "  inp_mask = cv2.morphologyEx(mask,cv2.MORPH_OPEN,cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (val, val)))\n",
        "  inp_mask = cv2.dilate(inp_mask,cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (val, val)))\n",
        "  return inp_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jMSka8gfxHB"
      },
      "source": [
        "# IMAGE-BLENDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kAqs7YrufzHI"
      },
      "outputs": [],
      "source": [
        "def image_blend(background, img, mask, x, y):\n",
        "    '''\n",
        "    Arguments:\n",
        "    background - background image in CV2 RGB format\n",
        "    img - image of object in CV2 RGB format\n",
        "    mask - mask of object in CV2 RGB format\n",
        "    x, y - coordinates of the center of the object image\n",
        "    0 < x < width of background\n",
        "    0 < y < height of background\n",
        "    \n",
        "    Function returns background with added object in CV2 RGB format\n",
        "    \n",
        "    CV2 RGB format is a numpy array with dimensions width x height x 3\n",
        "    '''\n",
        "    # print(background.shape, type(background))\n",
        "    # print(img.shape, type(img))\n",
        "    bg = background.squeeze(0).permute(1,2,0).detach().numpy()\n",
        "    \n",
        "    img = img.squeeze(0).permute(1,2,0).detach().numpy()\n",
        "    mask = np.invert(mask)\n",
        "    # bg = background.clone()\n",
        "    # bg = np.expand_dims(bg, 2)\n",
        "    # print(bg.shape, img.shape, mask.shape, x, y)\n",
        "    h_bg, w_bg = bg.shape[0], bg.shape[1]\n",
        "    h, w = img.shape[0], img.shape[1]\n",
        "    \n",
        "    x = x - int(w/2)\n",
        "    y = y - int(h/2)    \n",
        "    \n",
        "    # mask_boolean = mask[:,:,0] == 0\n",
        "    mask_boolean = mask[:,:] == 0\n",
        "    mask_rgb_boolean = np.stack([mask_boolean, mask_boolean, mask_boolean], axis=2)\n",
        "    \n",
        "    if x >= 0 and y >= 0:\n",
        "        h_part = h - max(0, y+h-h_bg) \n",
        "        w_part = w - max(0, x+w-w_bg) \n",
        "        bg[y:y+h_part, x:x+w_part, :] = bg[y:y+h_part, x:x+w_part, :] * ~mask_rgb_boolean[0:h_part, 0:w_part, :] + (img * mask_rgb_boolean)[0:h_part, 0:w_part, :]\n",
        "        \n",
        "    elif x < 0 and y < 0:\n",
        "        h_part = h + y\n",
        "        w_part = w + x\n",
        "        bg[0:0+h_part, 0:0+w_part, :] = bg[0:0+h_part, 0:0+w_part, :] * ~mask_rgb_boolean[h-h_part:h, w-w_part:w, :] + (img * mask_rgb_boolean)[h-h_part:h, w-w_part:w, :]\n",
        "        \n",
        "    elif x < 0 and y >= 0:\n",
        "        h_part = h - max(0, y+h-h_bg)\n",
        "        w_part = w + x\n",
        "        bg[y:y+h_part, 0:0+w_part, :] = bg[y:y+h_part, 0:0+w_part, :] * ~mask_rgb_boolean[0:h_part, w-w_part:w, :] + (img * mask_rgb_boolean)[0:h_part, w-w_part:w, :]\n",
        "        \n",
        "    elif x >= 0 and y < 0:\n",
        "        h_part = h + y\n",
        "        w_part = w - max(0, x+w-w_bg)\n",
        "        bg[0:0+h_part, x:x+w_part, :] = bg[0:0+h_part, x:x+w_part, :] * ~mask_rgb_boolean[h-h_part:h, 0:w_part, :] + (img * mask_rgb_boolean)[h-h_part:h, 0:w_part, :]\n",
        "    \n",
        "    return bg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnAHJx1if06b"
      },
      "source": [
        "# IN-PAINTING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0IR-lbEAf2zT"
      },
      "outputs": [],
      "source": [
        "net = importlib.import_module(\"in_painting.src.model.aotgan\")\n",
        "inpaint_model = net.InpaintGenerator([1,2,5,8], 8)\n",
        "inpaint_model.load_state_dict(torch.load(\"/content/in_painting/experiments/G0000000.pt\", map_location='cpu'))\n",
        "\n",
        "\n",
        "\n",
        "def postprocess(image):\n",
        "    image = torch.clamp(image, -1., 1.)\n",
        "    image = (image + 1) / 2.0 * 255.0\n",
        "    image = image.permute(1, 2, 0)\n",
        "    image = image.cpu().numpy().astype(np.uint8)\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "def inpaint(model, img, mask):\n",
        "    mask1 = T.ToTensor()(mask.astype(int)).unsqueeze(0)\n",
        "\n",
        "    image = T.ToTensor()(img)\n",
        "    image = (image * 2.0 - 1.0).unsqueeze(0)\n",
        "\n",
        "    image_masked = (image * (1 - mask1).float()) + mask1\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred_img = model(image_masked, mask1 )\n",
        "    \n",
        "    # Using altered mask after altered prediction\n",
        "    try:\n",
        "      comp_imgs = (1 - mask1) * image + mask1 * pred_img\n",
        "    except:\n",
        "      w,h = mask1.shape[2:]\n",
        "\n",
        "      pred_img = T.Resize((w,h))(pred_img)\n",
        "      comp_imgs = (1 - mask1) * image + mask1 * pred_img\n",
        "    comp_np = postprocess(comp_imgs[0])\n",
        "    return comp_np\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TPG8ql8f4YU"
      },
      "source": [
        "# MASK VALIDITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XCWrgN5ef6X1"
      },
      "outputs": [],
      "source": [
        "def check_mask_validity(mask):\n",
        "  w,h = mask.shape[0], mask.shape[1]\n",
        "\n",
        "  total_size = w*h\n",
        "  min_size = total_size*0.05\n",
        "  max_size = total_size*0.8\n",
        "\n",
        "  mask_size = mask[mask==255].shape[0]\n",
        "\n",
        "  if mask_size>=min_size and mask_size<=max_size:\n",
        "    return True\n",
        "    \n",
        "  else:\n",
        "    return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "booFyE5Uf8VK"
      },
      "source": [
        "# CLIP EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPujz08yf-Gp",
        "outputId": "0eb80787-033d-4894-9dbd-e17878d2e3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 51.7MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# Get the clip_model\n",
        "clip_model, _ = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "\n",
        "# Function to generate the clip embeddings\n",
        "def get_clip_embedding(img, input_type=\"otr\"):\n",
        "\n",
        "  if input_type==\"org\":\n",
        "    image = cv2.imread(img)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224,224))\n",
        "    image = T.ToTensor()(image.astype(int)).unsqueeze(0).to(device)\n",
        "  \n",
        "  else:\n",
        "    image = T.Resize(224)(img)\n",
        "  \n",
        "\n",
        "  with tr.no_grad():\n",
        "    image_features = clip_model.encode_image(image)\n",
        "\n",
        "  image_features = image_features.detach().cpu().numpy().flatten()\n",
        "\n",
        "  return image_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZuB_YQ-gAJL"
      },
      "source": [
        "# INFERENCE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7K8LGYl9gBzs"
      },
      "outputs": [],
      "source": [
        "def inference_save(original_prediction, inpaint_prediction, inpaint_displace_prediction, original_clip_embedding, clip_embedding, image_name, class_name, dpos):\n",
        "  global compiled_result_list, inpaint_count, inpaint_disp_count\n",
        "  save_dict = {}\n",
        "\n",
        "  save_dict[\"Image_ID\"] = image_name\n",
        "  try:\n",
        "    cn = \"_\".join([word.capitalize() for word in class_name.lower().split(\"_\")])\n",
        "  except:\n",
        "    cn = class_name\n",
        "  save_dict[\"Class_Name\"] = cn\n",
        "\n",
        "  # Save the original memorability scores\n",
        "  for i in range(5):\n",
        "    key_name = f'Original_Prediction_Memorability_Score_{i}'\n",
        "    save_dict[key_name] = original_prediction[i]\n",
        "  \n",
        "  save_dict[\"Original_Prediction_Clip_Embedding\"] = original_clip_embedding\n",
        "\n",
        "  # Save the inpaint memorability scores\n",
        "  for i in range(5):\n",
        "    key_name = f'Inpaint_Prediction_Memorability_Score_{i}'\n",
        "    save_dict[key_name] = inpaint_prediction[i]\n",
        "\n",
        "  save_dict[\"Inpaint_Prediction_Clip_Embedding\"] = clip_embedding[0]\n",
        "\n",
        "  # Save the inpaint+displacement memorability scores\n",
        "  for count, pos_name in enumerate(dpos.keys()):\n",
        "\n",
        "    for i in range(5):\n",
        "      key_name = f'Inpaint_w_Displacement_{pos_name}_Prediction_Memorability_Score_{i}'\n",
        "      save_dict[key_name] = inpaint_displace_prediction[pos_name][i]\n",
        "\n",
        "    key_name = f\"Inpaint_w_Displacement_{pos_name}_Clip_Embedding\"\n",
        "    save_dict[key_name] = clip_embedding[count+1]\n",
        "\n",
        "  # Append the current dictionary to the global list\n",
        "  compiled_result_list.append(save_dict)\n",
        "\n",
        "  # Save the original memorability heatmaps as images\n",
        "  %cd Memorability_Heatmaps_Results\n",
        "  !mkdir Original_Results\n",
        "  for mem_plot in range(14):\n",
        "    img = original_prediction[mem_plot+5]\n",
        "    plt.imsave(f\"/content/Memorability_Heatmaps_Results/Original_Results/Map_{mem_plot}.png\", img, dpi=500)\n",
        "\n",
        "  # Save the inpaint memorability heatmaps of the particular object mask as images\n",
        "  !mkdir Inpaint_Results_\"$inpaint_count\"\n",
        "  for mem_plot in range(14):\n",
        "    img = inpaint_prediction[mem_plot+5]\n",
        "    try:\n",
        "      plt.imsave(f\"/content/Memorability_Heatmaps_Results/Inpaint_Results_{inpaint_count}/Map_{mem_plot}.png\", img, dpi=500)\n",
        "    except:\n",
        "      !mkdir -p Memorability_Heatmaps_Results/Inpaint_Results_\"$inpaint_count\"\n",
        "      plt.imsave(f\"/content/Memorability_Heatmaps_Results/Inpaint_Results_{inpaint_count}/Map_{mem_plot}.png\", img, dpi=500)\n",
        "\n",
        "  inpaint_count+=1\n",
        "\n",
        "  # Save the inpaint + displacement memorability heatmaps of the particular object mask as images\n",
        "  !mkdir Inpaint_w_Displacement_Results_\"$inpaint_disp_count\"\n",
        "\n",
        "  for count, pos_name in enumerate(dpos.keys()):\n",
        "    !mkdir Inpaint_w_Displacement_Results_\"$inpaint_disp_count\"/\"$pos_name\"\n",
        "    for mem_plot in range(14):\n",
        "      img = inpaint_displace_prediction[pos_name][mem_plot+5]\n",
        "      try:\n",
        "        plt.imsave(f\"/content/Memorability_Heatmaps_Results/Inpaint_w_Displacement_Results_{inpaint_disp_count}/{pos_name}/Map_{mem_plot}.png\", img, dpi=500)\n",
        "      except:\n",
        "        !mkdir -p Memorability_Heatmaps_Results/Inpaint_w_Displacement_Results_\"$cn\"/\"$pos_name\"\n",
        "        plt.imsave(f\"/content/Memorability_Heatmaps_Results/Inpaint_w_Displacement_Results_{inpaint_disp_count}/{pos_name}/Map_{mem_plot}.png\", img, dpi=500)\n",
        "  \n",
        "  inpaint_disp_count+=1\n",
        "  %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5dNDgYwgFEf"
      },
      "source": [
        "# MAIN FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YfuBDM28gG1m"
      },
      "outputs": [],
      "source": [
        "# GENERAL SETTINGS\n",
        "\n",
        "# Set this to get results of a particular memorability score\n",
        "mem_score_type = 0\n",
        "\n",
        "# Set to True if object is to be removed only\n",
        "# Set to False if object is to be removed and then inserted at a different position\n",
        "remove = False\n",
        "\n",
        "# Set to True to alter mask i.e. open and dilated it\n",
        "alt_mask = True\n",
        "\n",
        "# # If the above is set to False, mention the position of the object to be placed in the image\n",
        "# # This is the position of the center of the object on the backgroung image\n",
        "# x = 200\n",
        "# y = 500\n",
        "\n",
        "# Set this value to open and dilate the mask \n",
        "open = 50\n",
        "\n",
        "# Save all predictions\n",
        "original_prediction = []\n",
        "# inpaint_prediction = defaultdict(list)\n",
        "# inpaint_displace_prediction = defaultdict(list)\n",
        "\n",
        "# List to store all results and save as csv\n",
        "compiled_result_list = []\n",
        "\n",
        "# Create a new folder to save the results\n",
        "!mkdir -p Memorability_Heatmaps_Results/Original_Results\n",
        "\n",
        "# Global variable to create folders for every mask\n",
        "inpaint_count = 0\n",
        "\n",
        "# Global variable to create folders for every mask\n",
        "inpaint_disp_count = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtokErSugIir"
      },
      "outputs": [],
      "source": [
        "def main(img_name, img_path=\"/content/\"):\n",
        "  global compiled_result_list\n",
        "  # Input image\n",
        "  image_path = img_path\n",
        "  img = cv2.imread(f'{image_path}{img_name}')\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "  # Dictionary to specify position \n",
        "  # w,h = img.shape[0], img.shape[1]\n",
        "  w, h = 456, 456\n",
        "  dpos = {}\n",
        "  dpos[\"Top_Left\"] = [int(w//4), int(h*0.75)]\n",
        "  dpos[\"Top_Right\"] = [int(w*0.75), int(h*0.75)]\n",
        "  dpos[\"Bottom_Left\"] = [int(w//4), int(h//4)]\n",
        "  dpos[\"Bottom_Right\"] = [int(w*0.75), int(h//4)]\n",
        "  dpos[\"Center\"] = [int(w//2), int(h//2)]\n",
        "\n",
        "  # Transform image to send to the memorability model \n",
        "  # _, model_input = ImageTransform(img, \"image\")\n",
        "\n",
        "  model_input = T.ToTensor()(img).unsqueeze(0)\n",
        "\n",
        "  # Call to object segmentation function\n",
        "  outputs, class_names = segmentation(f'{image_path}{img_name}', input_type=\"org\")\n",
        "\n",
        "  # Get original clip embeddings\n",
        "  original_clip_embedding = get_clip_embedding(img_name, \"org\")\n",
        "\n",
        "  # print(model_input.squeeze(0).permute(1,2,0).detach().cpu().numpy().shape)\n",
        "  plt.imsave(\"model_input.png\", model_input.squeeze(0).permute(1,2,0).detach().cpu().numpy())\n",
        "  _,mi = ImageTransform(\"model_input.png\")\n",
        "  # print(mi.shape)\n",
        "  model_output = model(mi.unsqueeze(0).cuda())\n",
        "\n",
        "  # Get output of the original model for all memorability scores\n",
        "  for mem_score_type in range(5):\n",
        "    original_prediction.append(model_output[0][0][mem_score_type].detach().cpu().numpy())\n",
        "\n",
        "  # Get output of the original model for all heatmap\n",
        "  for mem_plot in range(14):\n",
        "    original_prediction.append(model_output[1].detach().cpu().squeeze(0).permute(2,1,0).numpy()[:,:,mem_plot])\n",
        "\n",
        "  # Loop over all the masks\n",
        "  for idx, name in enumerate(class_names):\n",
        "    inpaint_prediction = []\n",
        "    inpaint_displace_prediction = defaultdict(list)\n",
        "\n",
        "    clip_embedding = []\n",
        "\n",
        "    mask = outputs[\"instances\"].to(\"cpu\").pred_masks[idx,:,:].numpy()\n",
        "    # mask_valid = check_mask_validity(mask)\n",
        "\n",
        "    # if mask_valid==False:\n",
        "    #   continue\n",
        "\n",
        "    # Alter the mask\n",
        "    # alt_mask = alter_mask(cv2.fromarray(mask), open)\n",
        "    alt_mask = alter_mask(mask, open)\n",
        "\n",
        "    # # INPAINT\n",
        "\n",
        "    # Inpaint the image with the altered mask\n",
        "    new_model_input = inpaint(inpaint_model, img, alt_mask)\n",
        "    new_model_input = T.ToTensor()(new_model_input.astype(int)).unsqueeze(0)\n",
        "\n",
        "    # Get the new model predictions\n",
        "    plt.imsave(\"model_input.png\", model_input.squeeze(0).permute(1,2,0).detach().cpu().numpy())\n",
        "    _,mi = ImageTransform(\"model_input.png\")\n",
        "    model_output = model(mi.unsqueeze(0).cuda())\n",
        "\n",
        "    key = f\"mask_{idx}\"\n",
        "    for mem_score_type in range(5):\n",
        "      # inpaint_prediction[key].append(model_output[0][:,mem_score_type])\n",
        "      # print(model_output[0][0][mem_score_type].detach().cpu().numpy())\n",
        "      inpaint_prediction.append(model_output[0][0][mem_score_type].detach().cpu().numpy())\n",
        "\n",
        "    for mem_plot in range(14):\n",
        "      # inpaint_prediction[key].append(model_output[1].detach().cpu().squeeze(0).permute(2,1,0).numpy()[:,:,mem_plot])\n",
        "      inpaint_prediction.append(model_output[1].detach().cpu().squeeze(0).permute(2,1,0).numpy()[:,:,mem_plot])\n",
        "\n",
        "    clip_embedding.append(get_clip_embedding(new_model_input.cuda()))\n",
        "\n",
        "    # # INPAINT + DISPLACEMENT\n",
        "\n",
        "    # Place object in specified position \n",
        "    for pos_name, pos in dpos.items():\n",
        "      # print(pos)\n",
        "      new_model_input = image_blend(new_model_input, model_input, mask, pos[0], pos[1])\n",
        "    \n",
        "      new_model_input = T.ToTensor()(new_model_input.astype(int)).unsqueeze(0)\n",
        "      plt.imsave(\"model_input.png\", model_input.squeeze(0).permute(1,2,0).detach().cpu().numpy())\n",
        "      _,mi = ImageTransform(\"model_input.png\")\n",
        "      model_output = model(mi.unsqueeze(0).cuda())\n",
        "\n",
        "      key = f\"mask_{idx}_{pos_name}\"\n",
        "      \n",
        "      for mem_score_type in range(5):\n",
        "        inpaint_displace_prediction[pos_name].append(model_output[0][0][mem_score_type].detach().cpu().numpy())\n",
        "\n",
        "      for mem_plot in range(14):\n",
        "        inpaint_displace_prediction[pos_name].append(model_output[1].detach().cpu().squeeze(0).permute(2,1,0).numpy()[:,:,mem_plot])\n",
        "\n",
        "      clip_embedding.append(get_clip_embedding(new_model_input.cuda()))\n",
        "\n",
        "    # Call to the inference script to update the results list with values for the current mask\n",
        "    inference_save(original_prediction, inpaint_prediction, inpaint_displace_prediction, original_clip_embedding, clip_embedding, img_name, name, dpos)\n",
        "\n",
        "  \n",
        "  # Save the tabular results to a csv file\n",
        "  df = pd.DataFrame.from_dict(compiled_result_list) \n",
        "  df.to_csv (r'Inference.csv', index = False, header=True)\n",
        "  !zip -r \"/content/Memorability_Heatmaps_Results.zip\" \"/content/Memorability_Heatmaps_Results\"\n",
        "\n",
        "img_name = \"input.jpg\"\n",
        "main(img_name)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}