{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0e835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os.path\n",
    "import cv2\n",
    "    \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Input, Concatenate, InputLayer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings \n",
    "warnings.filterwarnings('always')\n",
    "from skimage.transform import warp_polar, warp_coords\n",
    "# Helper code files\n",
    "import time\n",
    "sys.path.append('../')\n",
    "from Utils.utils import get_dataset, lp_architecture, set_model_weights\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3aa622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 1000\n",
    "batch_size = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f354f3e5-9021-419d-8fc1-17081bb0bddd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 14:56:34.389211: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-03 14:56:34.389349: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Exception encountered when calling layer \"log_polar_layer\" (type LogPolarLayer).\n\nin user code:\n\n    File \"/Users/varshini/Desktop/HARVARD/SPRING'22/MIT9.60-Human Vision/Project/Notebooks/../Utils/utils.py\", line 78, in call  *\n        func = lambda output : tf.squeeze(tf.image.rgb_to_grayscale(cartesian_to_spherical_coordinates(tf.image.grayscale_to_rgb(\n\n    NameError: name 'cartesian_to_spherical_coordinates' is not defined\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 150, 150, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_layer, output_layer \u001b[38;5;241m=\u001b[39m \u001b[43mlp_architecture\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_layer, outputs\u001b[38;5;241m=\u001b[39moutput_layer, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLP_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/HARVARD/SPRING'22/MIT9.60-Human Vision/Project/Notebooks/../Utils/utils.py:141\u001b[0m, in \u001b[0;36mlp_architecture\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m conv2 \u001b[38;5;241m=\u001b[39m Conv2D(filters \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, kernel_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    136\u001b[0m               name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv2\u001b[39m\u001b[38;5;124m'\u001b[39m)(conv1)\n\u001b[1;32m    138\u001b[0m maxpool1 \u001b[38;5;241m=\u001b[39m MaxPool2D(pool_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, strides \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    139\u001b[0m                     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxpool1\u001b[39m\u001b[38;5;124m'\u001b[39m)(conv2)\n\u001b[0;32m--> 141\u001b[0m lp \u001b[38;5;241m=\u001b[39m \u001b[43mLogPolarLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxpool1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m conv3 \u001b[38;5;241m=\u001b[39m Conv2D(filters \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, kernel_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    144\u001b[0m               name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv3\u001b[39m\u001b[38;5;124m'\u001b[39m)(lp)\n\u001b[1;32m    146\u001b[0m conv4 \u001b[38;5;241m=\u001b[39m Conv2D(filters \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, kernel_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    147\u001b[0m               name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv4\u001b[39m\u001b[38;5;124m'\u001b[39m)(conv3)\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: Exception encountered when calling layer \"log_polar_layer\" (type LogPolarLayer).\n\nin user code:\n\n    File \"/Users/varshini/Desktop/HARVARD/SPRING'22/MIT9.60-Human Vision/Project/Notebooks/../Utils/utils.py\", line 78, in call  *\n        func = lambda output : tf.squeeze(tf.image.rgb_to_grayscale(cartesian_to_spherical_coordinates(tf.image.grayscale_to_rgb(\n\n    NameError: name 'cartesian_to_spherical_coordinates' is not defined\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 150, 150, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "input_layer, output_layer = lp_architecture()\n",
    "model = Model(inputs=input_layer, outputs=output_layer, name=\"LP_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ca46b-0b08-4c1f-adc6-ef9de7e2a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = set_model_weights(model, model_type=\"custom\", \n",
    "                          path=\"HARVARD/SPRING'22/MIT9.60-Human Vision/Project/Notebooks/Model Weights/model_weights.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8553d-be05-494d-8056-6452d5ce310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT TAPE\n",
    "\n",
    "# Algorithm parameters\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "metrics = tf.keras.metrics.Accuracy()\n",
    "num_epochs = 30\n",
    "\n",
    "path=\"/Users/varshini/Desktop/HARVARD/SPRING'22/MIT9.60-Human Vision/Project/Data/Image_Data\"\n",
    "\n",
    "train_data = get_dataset(batch_size, False, path=path)\n",
    "\n",
    "# Since batch_size is 1, each epoch will see steps_per_epoch number of images\n",
    "steps_per_epoch = 128\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    print(\"\\nEPOCH:\", i+1)\n",
    "    loss = 0\n",
    "    \n",
    "    for bs in range(steps_per_epoch):\n",
    "        if bs%49==0:\n",
    "            print(\"\\tBATCH NUMBER:\",bs+1)\n",
    "        try:\n",
    "            t1 = train_data.next()\n",
    "        except:\n",
    "            print(\"\\t\\tFresh Images\")\n",
    "            train_data = get_dataset(batch_size, False,path=path)\n",
    "            t1 = train_data.next()\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model(t1[0])\n",
    "            loss+= loss_fn(t1[1], output)\n",
    "            \n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))    \n",
    "\n",
    "    model.save_weights(\"Model Weights/model_weights.h5\")\n",
    "\n",
    "    # clear_output(wait=True)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
